import 'dart:async';
import 'dart:convert';

import 'package:supabase_flutter/supabase_flutter.dart';

import '../../shared/models/relative_model.dart';
import '../services/ai_config_service.dart';
import '../services/app_logger_service.dart';
import 'ai_models.dart';
import 'ai_prompts.dart';
import 'ai_service.dart';

/// DeepSeek AI Service Implementation
/// Uses DeepSeek API via Supabase Edge Function proxy for security
class DeepSeekAIService implements AIService {
  static DeepSeekAIService? _instance;
  final AppLoggerService _logger = AppLoggerService();
  final SupabaseClient _supabase = Supabase.instance.client;

  // Edge function endpoint (proxy to DeepSeek)
  static const String _edgeFunctionName = 'deepseek-proxy';

  factory DeepSeekAIService() => _instance ??= DeepSeekAIService._internal();
  DeepSeekAIService._internal();

  @override
  Stream<AIStreamChunk> streamChatCompletion({
    required List<ChatMessage> messages,
    required String systemPrompt,
    double temperature = 0.7,
    int maxTokens = 2048,
  }) async* {
    try {
      // For now, use non-streaming and yield the result
      // TODO: Implement SSE streaming when edge function is ready
      final response = await getChatCompletion(
        messages: messages,
        systemPrompt: systemPrompt,
        temperature: temperature,
        maxTokens: maxTokens,
      );

      // Simulate GPT-like streaming by yielding word by word
      final words = _tokenizeForStreaming(response);
      for (final word in words) {
        yield AIStreamChunk(content: word);
        // Variable delay based on content
        final delay = _getStreamingDelay(word);
        await Future.delayed(Duration(milliseconds: delay));
      }

      yield AIStreamChunk(content: '', isDone: true);
    } on AIServiceException catch (e) {
      // Use the user-friendly message from AIServiceException
      yield AIStreamChunk(content: '', isDone: true, error: e.message);
    } catch (e) {
      // Fallback for unexpected errors
      yield AIStreamChunk(
        content: '',
        isDone: true,
        error: 'Ø­Ø¯Ø« Ø®Ø·Ø£ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹. ÙŠØ±Ø¬Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰.',
      );
    }
  }

  @override
  Future<String> getChatCompletion({
    required List<ChatMessage> messages,
    required String systemPrompt,
    double temperature = 0.7,
    int maxTokens = 2048,
    int? timeoutSeconds,
  }) async {
    try {
      final formattedMessages = [
        {'role': 'system', 'content': systemPrompt},
        ...messages.map((m) => m.toApiFormat()),
      ];

      // Use provided timeout or default from config (fallback to 60)
      final timeout = timeoutSeconds ?? 60;

      final response = await _supabase.functions.invoke(
        _edgeFunctionName,
        body: {
          'messages': formattedMessages,
          'temperature': temperature,
          'max_tokens': maxTokens,
        },
      ).timeout(
        Duration(seconds: timeout),
        onTimeout: () => throw AIServiceException(
          'Ø§Ù†ØªÙ‡Øª Ù…Ù‡Ù„Ø© Ø§Ù„Ø§ØªØµØ§Ù„. ÙŠØ±Ø¬Ù‰ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§ØªØµØ§Ù„ Ø§Ù„Ø¥Ù†ØªØ±Ù†Øª ÙˆØ§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰.',
          code: 'TIMEOUT',
        ),
      );

      if (response.status != 200) {
        throw AIServiceException(
          _getErrorMessage(response.status),
          code: response.status.toString(),
        );
      }

      final data = response.data as Map<String, dynamic>;
      final content = data['content'] as String? ?? '';

      // Handle empty response - throw error instead of returning blank
      if (content.trim().isEmpty) {
        throw AIServiceException(
          'Ù„Ù… ÙŠØªÙ…ÙƒÙ† Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯ Ù…Ù† Ø§Ù„Ø±Ø¯. ÙŠØ±Ø¬Ù‰ Ø¥Ø¹Ø§Ø¯Ø© ØµÙŠØ§ØºØ© Ø§Ù„Ø³Ø¤Ø§Ù„.',
          code: 'EMPTY_RESPONSE',
        );
      }

      return content;
    } catch (e, stackTrace) {
      _logger.error(
        'DeepSeek API error',
        category: LogCategory.network,
        tag: 'DeepSeekAIService',
        metadata: {'error': e.toString()},
        stackTrace: stackTrace,
      );

      if (e is AIServiceException) rethrow;

      // Handle FunctionException from Supabase
      final errorStr = e.toString();
      if (errorStr.contains('402') || errorStr.contains('Payment Required')) {
        throw AIServiceException(
          'Ø±ØµÙŠØ¯ Ø®Ø¯Ù…Ø© Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ØºÙŠØ± ÙƒØ§ÙÙ. ÙŠØ±Ø¬Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù„Ø§Ø­Ù‚Ø§Ù‹.',
          code: '402',
          originalError: e,
        );
      }
      if (errorStr.contains('429') || errorStr.contains('Too Many Requests')) {
        throw AIServiceException(
          'ØªÙ… ØªØ¬Ø§ÙˆØ² Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ Ù„Ù„Ø·Ù„Ø¨Ø§Øª. ÙŠØ±Ø¬Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ø¨Ø¹Ø¯ Ù‚Ù„ÙŠÙ„.',
          code: '429',
          originalError: e,
        );
      }
      if (errorStr.contains('503') || errorStr.contains('Service Unavailable')) {
        throw AIServiceException(
          'Ø§Ù„Ø®Ø¯Ù…Ø© ØºÙŠØ± Ù…ØªØ§Ø­Ø© Ø­Ø§Ù„ÙŠØ§Ù‹. ÙŠØ±Ø¬Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù„Ø§Ø­Ù‚Ø§Ù‹.',
          code: '503',
          originalError: e,
        );
      }

      throw AIServiceException(
        'Ø­Ø¯Ø« Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø§ØªØµØ§Ù„. ÙŠØ±Ø¬Ù‰ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø¥Ù†ØªØ±Ù†Øª ÙˆØ§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰.',
        originalError: e,
      );
    }
  }

  /// Get user-friendly Arabic error message based on HTTP status code
  /// Uses dynamic config from admin panel with fallback
  String _getErrorMessage(int statusCode) {
    return AIConfigService.instance.getErrorMessage(statusCode);
  }

  @override
  Future<CommunicationScript> getCommunicationScript({
    required String scenario,
    required Relative? relative,
    String? additionalContext,
  }) async {
    try {
      final params = AIConfigService.instance.getParametersFor('communication_script');
      final prompt = AIPrompts.communicationScriptPrompt(scenario, relative, additionalContext);
      final response = await getChatCompletion(
        messages: [
          ChatMessage(
            id: '',
            conversationId: '',
            userId: '',
            role: MessageRole.user,
            content: 'Ø³Ø§Ø¹Ø¯Ù†ÙŠ ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©',
            createdAt: DateTime.now(),
          ),
        ],
        systemPrompt: prompt,
        temperature: params.temperature,
        maxTokens: params.maxTokens,
        timeoutSeconds: params.timeoutSeconds,
      );

      // Parse JSON response
      final jsonMatch = RegExp(r'\{[\s\S]*\}').firstMatch(response);
      if (jsonMatch == null) {
        throw AIServiceException('Invalid response format');
      }

      final data = jsonDecode(jsonMatch.group(0)!) as Map<String, dynamic>;
      return CommunicationScript.fromJson(data);
    } catch (e) {
      _logger.error(
        'Communication script error',
        category: LogCategory.network,
        tag: 'DeepSeekAIService',
        metadata: {'error': e.toString()},
      );
      rethrow;
    }
  }

  @override
  Future<List<String>> generateMessages({
    required Relative relative,
    required String occasionType,
    required String tone,
    int count = 3,
  }) async {
    try {
      final config = AIConfigService.instance;
      final params = config.getParametersFor('message_generation');

      // Get occasion config for promptAddition
      final occasionConfig = config.messageOccasions
          .cast<AIMessageOccasion?>()
          .firstWhere((o) => o?.occasionKey == occasionType, orElse: () => null);

      // Get tone config for promptModifier
      final toneConfig = config.messageTones
          .cast<AIMessageTone?>()
          .firstWhere((t) => t?.toneKey == tone, orElse: () => null);

      final prompt = AIPrompts.messageGenerationPrompt(
        relative,
        occasionType,
        tone,
        occasionPromptAddition: occasionConfig?.promptAddition,
        tonePromptModifier: toneConfig?.promptModifier,
      );
      final response = await getChatCompletion(
        messages: [
          ChatMessage(
            id: '',
            conversationId: '',
            userId: '',
            role: MessageRole.user,
            content: 'Ø§ÙƒØªØ¨ Ø±Ø³Ø§Ø¦Ù„ Ù„Ù„Ù…Ù†Ø§Ø³Ø¨Ø©',
            createdAt: DateTime.now(),
          ),
        ],
        systemPrompt: prompt,
        temperature: params.temperature,
        maxTokens: params.maxTokens,
        timeoutSeconds: params.timeoutSeconds,
      );

      // Parse JSON response
      final jsonMatch = RegExp(r'\{[\s\S]*\}').firstMatch(response);
      if (jsonMatch == null) {
        throw AIServiceException('Invalid response format');
      }

      final data = jsonDecode(jsonMatch.group(0)!) as Map<String, dynamic>;
      final messages = (data['messages'] as List?)?.map((e) => e.toString()).toList() ?? [];

      return messages;
    } catch (e) {
      _logger.error(
        'Message generation error',
        category: LogCategory.network,
        tag: 'DeepSeekAIService',
        metadata: {'error': e.toString()},
      );
      rethrow;
    }
  }

  @override
  Future<RelationshipAnalysis> analyzeRelationship({
    required Relative relative,
  }) async {
    try {
      final params = AIConfigService.instance.getParametersFor('relationship_analysis');
      final prompt = AIPrompts.relationshipAnalysisPrompt(relative);
      final response = await getChatCompletion(
        messages: [
          ChatMessage(
            id: '',
            conversationId: '',
            userId: '',
            role: MessageRole.user,
            content: 'Ø­Ù„Ù„ Ù‡Ø°Ù‡ Ø§Ù„Ø¹Ù„Ø§Ù‚Ø© ÙˆÙ‚Ø¯Ù… Ù†ØµØ§Ø¦Ø­',
            createdAt: DateTime.now(),
          ),
        ],
        systemPrompt: prompt,
        temperature: params.temperature,
        maxTokens: params.maxTokens,
        timeoutSeconds: params.timeoutSeconds,
      );

      // Parse JSON response
      final jsonMatch = RegExp(r'\{[\s\S]*\}').firstMatch(response);
      if (jsonMatch == null) {
        throw AIServiceException('Invalid response format');
      }

      final data = jsonDecode(jsonMatch.group(0)!) as Map<String, dynamic>;
      return RelationshipAnalysis.fromJson(data);
    } catch (e) {
      _logger.error(
        'Relationship analysis error',
        category: LogCategory.network,
        tag: 'DeepSeekAIService',
        metadata: {'error': e.toString()},
      );
      // Return a fallback analysis
      return RelationshipAnalysis(
        summary: 'ØªØ¹Ø°Ø± ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¹Ù„Ø§Ù‚Ø©. ÙŠØ±Ø¬Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰.',
        insights: [],
        suggestions: [
          AnalysisSuggestion(
            icon: 'ğŸ“',
            title: 'ØªÙˆØ§ØµÙ„ Ù…Ø¹ ${relative.fullName}',
            description: 'Ø­Ø§ÙˆÙ„ Ø§Ù„ØªÙˆØ§ØµÙ„ Ø¨Ø´ÙƒÙ„ Ù…Ù†ØªØ¸Ù…',
            priority: 'medium',
          ),
        ],
        alerts: [],
      );
    }
  }

  @override
  Future<List<SmartReminderSuggestion>> getSmartReminderSuggestions({
    required List<Relative> relatives,
  }) async {
    if (relatives.isEmpty) return [];

    try {
      final params = AIConfigService.instance.getParametersFor('smart_reminders');
      final prompt = AIPrompts.smartReminderPrompt(relatives);
      final response = await getChatCompletion(
        messages: [
          ChatMessage(
            id: '',
            conversationId: '',
            userId: '',
            role: MessageRole.user,
            content: 'Ø§Ù‚ØªØ±Ø­ ØªØ°ÙƒÙŠØ±Ø§Øª Ù„Ù„ØªÙˆØ§ØµÙ„',
            createdAt: DateTime.now(),
          ),
        ],
        systemPrompt: prompt,
        temperature: params.temperature,
        maxTokens: params.maxTokens,
        timeoutSeconds: params.timeoutSeconds,
      );

      // Parse JSON response
      final jsonMatch = RegExp(r'\{[\s\S]*\}').firstMatch(response);
      if (jsonMatch == null) {
        throw AIServiceException('Invalid response format');
      }

      final data = jsonDecode(jsonMatch.group(0)!) as Map<String, dynamic>;
      final suggestions = (data['suggestions'] as List?)
              ?.map((e) => SmartReminderSuggestion.fromJson(e as Map<String, dynamic>))
              .toList() ??
          [];

      return suggestions;
    } catch (e) {
      _logger.error(
        'Smart reminder error',
        category: LogCategory.network,
        tag: 'DeepSeekAIService',
        metadata: {'error': e.toString()},
      );
      return [];
    }
  }

  @override
  Future<bool> isAvailable() async {
    try {
      // Check if edge function is deployed
      final response = await _supabase.functions.invoke(
        _edgeFunctionName,
        body: {'health_check': true},
      );
      return response.status == 200;
    } catch (e) {
      return false;
    }
  }

  @override
  void dispose() {
    // Nothing to dispose currently
  }

  /// Tokenize text for natural streaming (word by word with punctuation)
  List<String> _tokenizeForStreaming(String text) {
    final tokens = <String>[];
    final buffer = StringBuffer();

    for (var i = 0; i < text.length; i++) {
      final char = text[i];

      if (char == ' ') {
        if (buffer.isNotEmpty) {
          tokens.add(buffer.toString());
          buffer.clear();
        }
        tokens.add(' ');
      } else if (_isPunctuation(char)) {
        if (buffer.isNotEmpty) {
          tokens.add(buffer.toString());
          buffer.clear();
        }
        tokens.add(char);
      } else if (char == '\n') {
        if (buffer.isNotEmpty) {
          tokens.add(buffer.toString());
          buffer.clear();
        }
        tokens.add('\n');
      } else {
        buffer.write(char);
      }
    }

    if (buffer.isNotEmpty) {
      tokens.add(buffer.toString());
    }

    return tokens;
  }

  /// Check if character is punctuation
  bool _isPunctuation(String char) {
    return ['.', 'ØŒ', 'ØŸ', '!', ':', 'Ø›', '-', '*', '#', ')', '(', '"', '\'']
        .contains(char);
  }

  /// Get variable delay for natural typing effect (ultra-fast, ChatGPT-like)
  /// Uses dynamic config from admin panel with fallback
  int _getStreamingDelay(String token) {
    return AIConfigService.instance.streamingConfig.getDelayForToken(token);
  }

  @override
  Future<List<Map<String, dynamic>>> extractMemories(String conversation) async {
    try {
      final params = AIConfigService.instance.getParametersFor('memory_extraction');
      final response = await getChatCompletion(
        messages: [
          ChatMessage(
            id: 'extract',
            conversationId: '',
            userId: '',
            role: MessageRole.user,
            content: conversation,
            createdAt: DateTime.now(),
          ),
        ],
        systemPrompt: AIPrompts.memoryExtractionPrompt,
        temperature: params.temperature,
        maxTokens: params.maxTokens,
        timeoutSeconds: params.timeoutSeconds,
      );

      // Parse JSON response
      try {
        // Clean up response - remove markdown code blocks if present
        String cleanResponse = response.trim();
        if (cleanResponse.startsWith('```json')) {
          cleanResponse = cleanResponse.substring(7);
        } else if (cleanResponse.startsWith('```')) {
          cleanResponse = cleanResponse.substring(3);
        }
        if (cleanResponse.endsWith('```')) {
          cleanResponse = cleanResponse.substring(0, cleanResponse.length - 3);
        }
        cleanResponse = cleanResponse.trim();

        final json = jsonDecode(cleanResponse);
        final memories = json['memories'] as List<dynamic>?;
        if (memories == null || memories.isEmpty) {
          return [];
        }
        return memories.cast<Map<String, dynamic>>();
      } catch (parseError) {
        _logger.warning(
          'Failed to parse memory extraction response',
          category: LogCategory.network,
          tag: 'ExtractMemories',
          metadata: {'response': response, 'error': parseError.toString()},
        );
        return [];
      }
    } catch (e) {
      _logger.warning(
        'Memory extraction failed',
        category: LogCategory.network,
        tag: 'ExtractMemories',
        metadata: {'error': e.toString()},
      );
      return [];
    }
  }
}

/// Mock AI Service for testing and development without API key
class MockAIService implements AIService {
  @override
  Stream<AIStreamChunk> streamChatCompletion({
    required List<ChatMessage> messages,
    required String systemPrompt,
    double temperature = 0.7,
    int maxTokens = 2048,
  }) async* {
    const mockResponse = '''
Ø£Ù‡Ù„Ø§Ù‹ Ø¨Ùƒ! Ø£Ù†Ø§ ÙˆØ§ØµÙ„ØŒ Ù…Ø³Ø§Ø¹Ø¯Ùƒ ÙÙŠ ØµÙ„Ø© Ø§Ù„Ø±Ø­Ù….

ØµÙ„Ø© Ø§Ù„Ø±Ø­Ù… Ù…Ù† Ø£Ø¹Ø¸Ù… Ø§Ù„Ø£Ø¹Ù…Ø§Ù„ Ø¹Ù†Ø¯ Ø§Ù„Ù„Ù‡ØŒ ÙˆÙ‡ÙŠ Ø³Ø¨Ø¨ Ù„Ù„Ø¨Ø±ÙƒØ© ÙÙŠ Ø§Ù„Ø±Ø²Ù‚ ÙˆØ§Ù„Ø¹Ù…Ø±.

ÙƒÙŠÙ ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ Ø§Ù„ÙŠÙˆÙ… ÙÙŠ Ø§Ù„ØªÙˆØ§ØµÙ„ Ù…Ø¹ Ø£Ù‚Ø§Ø±Ø¨ÙƒØŸ
''';

    for (var i = 0; i < mockResponse.length; i += 10) {
      final end = (i + 10 < mockResponse.length) ? i + 10 : mockResponse.length;
      yield AIStreamChunk(content: mockResponse.substring(i, end));
      await Future.delayed(const Duration(milliseconds: 50));
    }
    yield AIStreamChunk(content: '', isDone: true);
  }

  @override
  Future<String> getChatCompletion({
    required List<ChatMessage> messages,
    required String systemPrompt,
    double temperature = 0.7,
    int maxTokens = 2048,
    int? timeoutSeconds,
  }) async {
    await Future.delayed(const Duration(milliseconds: 500));
    return '''
Ø£Ù‡Ù„Ø§Ù‹ Ø¨Ùƒ! Ø£Ù†Ø§ ÙˆØ§ØµÙ„ØŒ Ù…Ø³Ø§Ø¹Ø¯Ùƒ ÙÙŠ ØµÙ„Ø© Ø§Ù„Ø±Ø­Ù….

ØµÙ„Ø© Ø§Ù„Ø±Ø­Ù… Ù…Ù† Ø£Ø¹Ø¸Ù… Ø§Ù„Ø£Ø¹Ù…Ø§Ù„ Ø¹Ù†Ø¯ Ø§Ù„Ù„Ù‡ØŒ ÙˆÙ‡ÙŠ Ø³Ø¨Ø¨ Ù„Ù„Ø¨Ø±ÙƒØ© ÙÙŠ Ø§Ù„Ø±Ø²Ù‚ ÙˆØ§Ù„Ø¹Ù…Ø±.

ÙƒÙŠÙ ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ Ø§Ù„ÙŠÙˆÙ… ÙÙŠ Ø§Ù„ØªÙˆØ§ØµÙ„ Ù…Ø¹ Ø£Ù‚Ø§Ø±Ø¨ÙƒØŸ
''';
  }

  @override
  Future<CommunicationScript> getCommunicationScript({
    required String scenario,
    required Relative? relative,
    String? additionalContext,
  }) async {
    await Future.delayed(const Duration(milliseconds: 500));
    return CommunicationScript(
      opening: 'Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ…ØŒ ÙƒÙŠÙ Ø­Ø§Ù„ÙƒØŸ Ø£ØªÙ…Ù†Ù‰ Ø£Ù† ØªÙƒÙˆÙ† Ø¨Ø®ÙŠØ±',
      keyPoints: [
        'Ø¹Ø¨Ù‘Ø± Ø¹Ù† Ø§Ø´ØªÙŠØ§Ù‚Ùƒ',
        'Ø§Ø³Ø£Ù„ Ø¹Ù† Ø£Ø­ÙˆØ§Ù„Ù‡Ù…',
        'Ø§Ù‚ØªØ±Ø­ Ù„Ù‚Ø§Ø¡ Ù‚Ø±ÙŠØ¨',
      ],
      phrasesToUse: [
        'ÙˆØ­Ø´ØªÙ†ÙŠ ÙƒØ«ÙŠØ±Ø§Ù‹',
        'Ø£ØªÙ…Ù†Ù‰ Ø£Ù† Ù†Ù„ØªÙ‚ÙŠ Ù‚Ø±ÙŠØ¨Ø§Ù‹',
        'Ø£Ù†Øª ÙÙŠ Ø¨Ø§Ù„ÙŠ Ø¯Ø§Ø¦Ù…Ø§Ù‹',
      ],
      phrasesToAvoid: [
        'Ù„Ù…Ø§Ø°Ø§ Ù„Ù… ØªØªØµÙ„ØŸ',
        'Ø£Ù†Øª Ø¯Ø§Ø¦Ù…Ø§Ù‹ Ù…Ø´ØºÙˆÙ„',
      ],
      closing: 'Ø£Ø­Ø¨Ùƒ ÙÙŠ Ø§Ù„Ù„Ù‡ØŒ ÙˆØ£ØªØ·Ù„Ø¹ Ù„Ø±Ø¤ÙŠØªÙƒ Ù‚Ø±ÙŠØ¨Ø§Ù‹',
    );
  }

  @override
  Future<List<String>> generateMessages({
    required Relative relative,
    required String occasionType,
    required String tone,
    int count = 3,
  }) async {
    await Future.delayed(const Duration(milliseconds: 500));
    return [
      'ÙƒÙ„ Ø¹Ø§Ù… ÙˆØ£Ù†Øª Ø¨Ø®ÙŠØ±ØŒ Ø£Ø³Ø£Ù„ Ø§Ù„Ù„Ù‡ Ø£Ù† ÙŠØ­ÙØ¸Ùƒ ÙˆÙŠØ¨Ø§Ø±Ùƒ ÙÙŠÙƒ',
      'Ø£Ù‡Ù†Ø¦Ùƒ Ù…Ù† ÙƒÙ„ Ù‚Ù„Ø¨ÙŠØŒ ÙˆØ£ØªÙ…Ù†Ù‰ Ù„Ùƒ Ø§Ù„Ø³Ø¹Ø§Ø¯Ø© ÙˆØ§Ù„ØªÙˆÙÙŠÙ‚',
      'Ù…Ø¨Ø§Ø±Ùƒ Ø¹Ù„ÙŠÙƒØŒ ÙˆØ£Ø³Ø£Ù„ Ø§Ù„Ù„Ù‡ Ø£Ù† ÙŠØ¬Ø¹Ù„Ù‡Ø§ Ø£ÙŠØ§Ù…Ø§Ù‹ Ø³Ø¹ÙŠØ¯Ø©',
    ];
  }

  @override
  Future<RelationshipAnalysis> analyzeRelationship({
    required Relative relative,
  }) async {
    await Future.delayed(const Duration(milliseconds: 500));
    return RelationshipAnalysis(
      summary: 'Ø§Ù„Ø¹Ù„Ø§Ù‚Ø© Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ Ø§Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„ØªÙˆØ§ØµÙ„ Ø§Ù„Ù…Ù†ØªØ¸Ù…',
      insights: [
        AnalysisInsight(
          icon: 'ğŸ’¡',
          title: 'ÙØ±ØµØ© Ù„Ù„ØªÙ‚Ø§Ø±Ø¨',
          description: 'ÙŠÙ…ÙƒÙ†Ùƒ ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø¹Ù„Ø§Ù‚Ø© Ù…Ù† Ø®Ù„Ø§Ù„ Ø§Ù„ØªÙˆØ§ØµÙ„ Ø§Ù„Ù…Ø³ØªÙ…Ø±',
        ),
      ],
      suggestions: [
        AnalysisSuggestion(
          icon: 'ğŸ“',
          title: 'Ø§ØªØµÙ„ Ø§Ù„ÙŠÙˆÙ…',
          description: 'Ù…ÙƒØ§Ù„Ù…Ø© Ù‚ØµÙŠØ±Ø© ØªØµÙ†Ø¹ Ø§Ù„ÙØ±Ù‚',
          priority: 'high',
        ),
        AnalysisSuggestion(
          icon: 'ğŸ’¬',
          title: 'Ø£Ø±Ø³Ù„ Ø±Ø³Ø§Ù„Ø©',
          description: 'Ø±Ø³Ø§Ù„Ø© Ø¨Ø³ÙŠØ·Ø© Ù„Ù„Ø§Ø·Ù…Ø¦Ù†Ø§Ù†',
          priority: 'medium',
        ),
      ],
      alerts: [],
    );
  }

  @override
  Future<List<SmartReminderSuggestion>> getSmartReminderSuggestions({
    required List<Relative> relatives,
  }) async {
    await Future.delayed(const Duration(milliseconds: 500));
    if (relatives.isEmpty) return [];

    return [
      SmartReminderSuggestion(
        relativeName: relatives.first.fullName,
        reason: 'Ù…Ø¶Ù‰ ÙˆÙ‚Øª Ø¹Ù„Ù‰ Ø¢Ø®Ø± ØªÙˆØ§ØµÙ„',
        urgency: 'medium',
        suggestedAction: 'Ø±Ø³Ø§Ù„Ø©',
        suggestedMessage: 'Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ…ØŒ ÙƒÙŠÙ Ø­Ø§Ù„ÙƒØŸ',
      ),
    ];
  }

  @override
  Future<List<Map<String, dynamic>>> extractMemories(String conversation) async {
    // Mock implementation - returns empty list
    return [];
  }

  @override
  Future<bool> isAvailable() async => true;

  @override
  void dispose() {}
}
